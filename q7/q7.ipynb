{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1670428850387,
     "user": {
      "displayName": "01fe20bec069",
      "userId": "03138269682349277701"
     },
     "user_tz": -330
    },
    "id": "vc7E-JVpUKeR",
    "outputId": "c287f64e-f447-48c6-aa9e-dd1ee06a1784"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial hidden weights: [0.05890607 0.95115389] [0.31218255 0.26980217]\n",
      "Initial hidden biases: [0.2942729  0.00256949]\n",
      "Initial output weights: [0.84730491] [0.70870805]\n",
      "Initial output biases: [0.447846]\n",
      "Final hidden weights: [3.65296766 5.78111557] [3.65607943 5.79739539]\n",
      "Final hidden bias: [-5.58773496 -2.38757846]\n",
      "Final output weights: [-7.97081115] [7.36778803]\n",
      "Final output bias: [-3.31941741]\n",
      "\n",
      "Output from neural network after 10,000 epochs: [0.06127102] [0.94287571] [0.9428216] [0.06215993]\n"
     ]
    }
   ],
   "source": [
    "# for xor gate\n",
    "\n",
    "import numpy as np \n",
    "#np.random.seed(0)\n",
    "\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "#Input datasets\n",
    "inputs = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "expected_output = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "epochs = 10000\n",
    "lr = 0.1\n",
    "inputLayerNeurons, hiddenLayerNeurons, outputLayerNeurons = 2,2,1\n",
    "\n",
    "#Random weights and bias initialization\n",
    "hidden_weights = np.random.uniform(size=(inputLayerNeurons,hiddenLayerNeurons))\n",
    "hidden_bias =np.random.uniform(size=(1,hiddenLayerNeurons))\n",
    "output_weights = np.random.uniform(size=(hiddenLayerNeurons,outputLayerNeurons))\n",
    "output_bias = np.random.uniform(size=(1,outputLayerNeurons))\n",
    "\n",
    "print(\"Initial hidden weights: \",end='')\n",
    "print(*hidden_weights)\n",
    "print(\"Initial hidden biases: \",end='')\n",
    "print(*hidden_bias)\n",
    "print(\"Initial output weights: \",end='')\n",
    "print(*output_weights)\n",
    "print(\"Initial output biases: \",end='')\n",
    "print(*output_bias)\n",
    "\n",
    "\n",
    "#Training algorithm\n",
    "for _ in range(epochs):\n",
    "\t#Forward Propagation\n",
    "\thidden_layer_activation = np.dot(inputs,hidden_weights)\n",
    "\thidden_layer_activation += hidden_bias\n",
    "\thidden_layer_output = sigmoid(hidden_layer_activation)\n",
    "\n",
    "\toutput_layer_activation = np.dot(hidden_layer_output,output_weights)\n",
    "\toutput_layer_activation += output_bias\n",
    "\tpredicted_output = sigmoid(output_layer_activation)\n",
    "\n",
    "\t#Backpropagation\n",
    "\terror = expected_output - predicted_output\n",
    "\td_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "\t\n",
    "\terror_hidden_layer = d_predicted_output.dot(output_weights.T)\n",
    "\td_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "\t#Updating Weights and Biases\n",
    "\toutput_weights += hidden_layer_output.T.dot(d_predicted_output) * lr\n",
    "\toutput_bias += np.sum(d_predicted_output,axis=0,keepdims=True) * lr\n",
    "\thidden_weights += inputs.T.dot(d_hidden_layer) * lr\n",
    "\thidden_bias += np.sum(d_hidden_layer,axis=0,keepdims=True) * lr\n",
    "\n",
    "print(\"Final hidden weights: \",end='')\n",
    "print(*hidden_weights)\n",
    "print(\"Final hidden bias: \",end='')\n",
    "print(*hidden_bias)\n",
    "print(\"Final output weights: \",end='')\n",
    "print(*output_weights)\n",
    "print(\"Final output bias: \",end='')\n",
    "print(*output_bias)\n",
    "\n",
    "print(\"\\nOutput from neural network after 10,000 epochs: \",end='')\n",
    "print(*predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8668,
     "status": "ok",
     "timestamp": 1670430406378,
     "user": {
      "displayName": "01fe20bec069",
      "userId": "03138269682349277701"
     },
     "user_tz": -330
    },
    "id": "iy-Vi67zYMF-",
    "outputId": "ffe1e620-b218-4d11-b4b9-2d389eab0543"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# for nand gate\n",
    "\n",
    "# import Python Libraries\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Sigmoid Function\n",
    "def sigmoid(z):\n",
    "\treturn 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Initialization of the neural network parameters\n",
    "# Initialized all the weights in the range of between 0 and 1\n",
    "# Bias values are initialized to 0\n",
    "def initializeParameters(inputFeatures, neuronsInHiddenLayers, outputFeatures):\n",
    "\tW1 = np.random.randn(neuronsInHiddenLayers, inputFeatures)\n",
    "\tW2 = np.random.randn(outputFeatures, neuronsInHiddenLayers)\n",
    "\tb1 = np.zeros((neuronsInHiddenLayers, 1))\n",
    "\tb2 = np.zeros((outputFeatures, 1))\n",
    "\t\n",
    "\tparameters = {\"W1\" : W1, \"b1\": b1,\n",
    "\t\t\t\t\"W2\" : W2, \"b2\": b2}\n",
    "\treturn parameters\n",
    "\n",
    "# Forward Propagation\n",
    "def forwardPropagation(X, Y, parameters):\n",
    "\tm = X.shape[1]\n",
    "\tW1 = parameters[\"W1\"]\n",
    "\tW2 = parameters[\"W2\"]\n",
    "\tb1 = parameters[\"b1\"]\n",
    "\tb2 = parameters[\"b2\"]\n",
    "\n",
    "\tZ1 = np.dot(W1, X) + b1\n",
    "\tA1 = sigmoid(Z1)\n",
    "\tZ2 = np.dot(W2, A1) + b2\n",
    "\tA2 = sigmoid(Z2)\n",
    "\n",
    "\tcache = (Z1, A1, W1, b1, Z2, A2, W2, b2)\n",
    "\tlogprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), (1 - Y))\n",
    "\tcost = -np.sum(logprobs) / m\n",
    "\treturn cost, cache, A2\n",
    "\n",
    "# Backward Propagation\n",
    "def backwardPropagation(X, Y, cache):\n",
    "\tm = X.shape[1]\n",
    "\t(Z1, A1, W1, b1, Z2, A2, W2, b2) = cache\n",
    "\t\n",
    "\tdZ2 = A2 - Y\n",
    "\tdW2 = np.dot(dZ2, A1.T) / m\n",
    "\tdb2 = np.sum(dZ2, axis = 1, keepdims = True)\n",
    "\t\n",
    "\tdA1 = np.dot(W2.T, dZ2)\n",
    "\tdZ1 = np.multiply(dA1, A1 * (1- A1))\n",
    "\tdW1 = np.dot(dZ1, X.T) / m\n",
    "\tdb1 = np.sum(dZ1, axis = 1, keepdims = True) / m\n",
    "\t\n",
    "\tgradients = {\"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "\t\t\t\t\"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "\treturn gradients\n",
    "\n",
    "# Updating the weights based on the negative gradients\n",
    "def updateParameters(parameters, gradients, learningRate):\n",
    "\tparameters[\"W1\"] = parameters[\"W1\"] - learningRate * gradients[\"dW1\"]\n",
    "\tparameters[\"W2\"] = parameters[\"W2\"] - learningRate * gradients[\"dW2\"]\n",
    "\tparameters[\"b1\"] = parameters[\"b1\"] - learningRate * gradients[\"db1\"]\n",
    "\tparameters[\"b2\"] = parameters[\"b2\"] - learningRate * gradients[\"db2\"]\n",
    "\treturn parameters\n",
    "\n",
    "# Model to learn the NAND truth table\n",
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]]) # NAND input\n",
    "Y = np.array([[1, 1, 1, 0]]) # NAND output\n",
    "\n",
    "# Define model parameters\n",
    "neuronsInHiddenLayers = 2 # number of hidden layer neurons (2)\n",
    "inputFeatures = X.shape[0] # number of input features (2)\n",
    "outputFeatures = Y.shape[0] # number of output features (1)\n",
    "parameters = initializeParameters(inputFeatures, neuronsInHiddenLayers, outputFeatures)\n",
    "epoch = 100000\n",
    "learningRate = 0.01\n",
    "losses = np.zeros((epoch, 1))\n",
    "\n",
    "for i in range(epoch):\n",
    "\tlosses[i, 0], cache, A2 = forwardPropagation(X, Y, parameters)\n",
    "\tgradients = backwardPropagation(X, Y, cache)\n",
    "\tparameters = updateParameters(parameters, gradients, learningRate)\n",
    "\n",
    "# # Evaluating the performance\n",
    "# plt.figure()\n",
    "# plt.plot(losses)\n",
    "# plt.xlabel(\"EPOCHS\")\n",
    "# plt.ylabel(\"Loss value\")\n",
    "# plt.show()\n",
    "\n",
    "# Testing\n",
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]]) # NAND input\n",
    "cost, _, A2 = forwardPropagation(X, Y, parameters)\n",
    "prediction = (A2 > 0.5) * 1.0\n",
    "\n",
    "# print(A2)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8412,
     "status": "ok",
     "timestamp": 1670430483136,
     "user": {
      "displayName": "01fe20bec069",
      "userId": "03138269682349277701"
     },
     "user_tz": -330
    },
    "id": "fAnngroLZ9O4",
    "outputId": "bece01fe-c715-4687-f45f-fc400adec086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#xnor gate\n",
    "\n",
    "\n",
    "# import Python Libraries\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Sigmoid Function\n",
    "def sigmoid(z):\n",
    "\treturn 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Initialization of the neural network parameters\n",
    "# Initialized all the weights in the range of between 0 and 1\n",
    "# Bias values are initialized to 0\n",
    "def initializeParameters(inputFeatures, neuronsInHiddenLayers, outputFeatures):\n",
    "\tW1 = np.random.randn(neuronsInHiddenLayers, inputFeatures)\n",
    "\tW2 = np.random.randn(outputFeatures, neuronsInHiddenLayers)\n",
    "\tb1 = np.zeros((neuronsInHiddenLayers, 1))\n",
    "\tb2 = np.zeros((outputFeatures, 1))\n",
    "\t\n",
    "\tparameters = {\"W1\" : W1, \"b1\": b1,\n",
    "\t\t\t\t\"W2\" : W2, \"b2\": b2}\n",
    "\treturn parameters\n",
    "\n",
    "# Forward Propagation\n",
    "def forwardPropagation(X, Y, parameters):\n",
    "\tm = X.shape[1]\n",
    "\tW1 = parameters[\"W1\"]\n",
    "\tW2 = parameters[\"W2\"]\n",
    "\tb1 = parameters[\"b1\"]\n",
    "\tb2 = parameters[\"b2\"]\n",
    "\n",
    "\tZ1 = np.dot(W1, X) + b1\n",
    "\tA1 = sigmoid(Z1)\n",
    "\tZ2 = np.dot(W2, A1) + b2\n",
    "\tA2 = sigmoid(Z2)\n",
    "\n",
    "\tcache = (Z1, A1, W1, b1, Z2, A2, W2, b2)\n",
    "\tlogprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), (1 - Y))\n",
    "\tcost = -np.sum(logprobs) / m\n",
    "\treturn cost, cache, A2\n",
    "\n",
    "# Backward Propagation\n",
    "def backwardPropagation(X, Y, cache):\n",
    "\tm = X.shape[1]\n",
    "\t(Z1, A1, W1, b1, Z2, A2, W2, b2) = cache\n",
    "\t\n",
    "\tdZ2 = A2 - Y\n",
    "\tdW2 = np.dot(dZ2, A1.T) / m\n",
    "\tdb2 = np.sum(dZ2, axis = 1, keepdims = True)\n",
    "\t\n",
    "\tdA1 = np.dot(W2.T, dZ2)\n",
    "\tdZ1 = np.multiply(dA1, A1 * (1- A1))\n",
    "\tdW1 = np.dot(dZ1, X.T) / m\n",
    "\tdb1 = np.sum(dZ1, axis = 1, keepdims = True) / m\n",
    "\t\n",
    "\tgradients = {\"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "\t\t\t\t\"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "\treturn gradients\n",
    "\n",
    "# Updating the weights based on the negative gradients\n",
    "def updateParameters(parameters, gradients, learningRate):\n",
    "\tparameters[\"W1\"] = parameters[\"W1\"] - learningRate * gradients[\"dW1\"]\n",
    "\tparameters[\"W2\"] = parameters[\"W2\"] - learningRate * gradients[\"dW2\"]\n",
    "\tparameters[\"b1\"] = parameters[\"b1\"] - learningRate * gradients[\"db1\"]\n",
    "\tparameters[\"b2\"] = parameters[\"b2\"] - learningRate * gradients[\"db2\"]\n",
    "\treturn parameters\n",
    "\n",
    "# Model to learn the XNOR truth table\n",
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]]) # XNOR input\n",
    "Y = np.array([[1, 0, 0, 1]]) # XNOR output\n",
    "\n",
    "# Define model parameters\n",
    "neuronsInHiddenLayers = 2 # number of hidden layer neurons (2)\n",
    "inputFeatures = X.shape[0] # number of input features (2)\n",
    "outputFeatures = Y.shape[0] # number of output features (1)\n",
    "parameters = initializeParameters(inputFeatures, neuronsInHiddenLayers, outputFeatures)\n",
    "epoch = 100000\n",
    "learningRate = 0.01\n",
    "losses = np.zeros((epoch, 1))\n",
    "\n",
    "for i in range(epoch):\n",
    "\tlosses[i, 0], cache, A2 = forwardPropagation(X, Y, parameters)\n",
    "\tgradients = backwardPropagation(X, Y, cache)\n",
    "\tparameters = updateParameters(parameters, gradients, learningRate)\n",
    "\n",
    "# Evaluating the performance\n",
    "# plt.figure()\n",
    "# plt.plot(losses)\n",
    "# plt.xlabel(\"EPOCHS\")\n",
    "# plt.ylabel(\"Loss value\")\n",
    "# plt.show()\n",
    "\n",
    "# Testing\n",
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]]) # XNOR input\n",
    "cost, _, A2 = forwardPropagation(X, Y, parameters)\n",
    "prediction = (A2 > 0.5) * 1.0\n",
    "# print(A2)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8612,
     "status": "ok",
     "timestamp": 1670430744348,
     "user": {
      "displayName": "01fe20bec069",
      "userId": "03138269682349277701"
     },
     "user_tz": -330
    },
    "id": "vC_2Lpy4aAK9",
    "outputId": "921cca2e-5485-4194-e2c9-4a444515e272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#alternate xor gate\n",
    "\n",
    "# import Python Libraries\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Sigmoid Function\n",
    "def sigmoid(z):\n",
    "\treturn 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Initialization of the neural network parameters\n",
    "# Initialized all the weights in the range of between 0 and 1\n",
    "# Bias values are initialized to 0\n",
    "def initializeParameters(inputFeatures, neuronsInHiddenLayers, outputFeatures):\n",
    "\tW1 = np.random.randn(neuronsInHiddenLayers, inputFeatures)\n",
    "\tW2 = np.random.randn(outputFeatures, neuronsInHiddenLayers)\n",
    "\tb1 = np.zeros((neuronsInHiddenLayers, 1))\n",
    "\tb2 = np.zeros((outputFeatures, 1))\n",
    "\t\n",
    "\tparameters = {\"W1\" : W1, \"b1\": b1,\n",
    "\t\t\t\t\"W2\" : W2, \"b2\": b2}\n",
    "\treturn parameters\n",
    "\n",
    "# Forward Propagation\n",
    "def forwardPropagation(X, Y, parameters):\n",
    "\tm = X.shape[1]\n",
    "\tW1 = parameters[\"W1\"]\n",
    "\tW2 = parameters[\"W2\"]\n",
    "\tb1 = parameters[\"b1\"]\n",
    "\tb2 = parameters[\"b2\"]\n",
    "\n",
    "\tZ1 = np.dot(W1, X) + b1\n",
    "\tA1 = sigmoid(Z1)\n",
    "\tZ2 = np.dot(W2, A1) + b2\n",
    "\tA2 = sigmoid(Z2)\n",
    "\n",
    "\tcache = (Z1, A1, W1, b1, Z2, A2, W2, b2)\n",
    "\tlogprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), (1 - Y))\n",
    "\tcost = -np.sum(logprobs) / m\n",
    "\treturn cost, cache, A2\n",
    "\n",
    "# Backward Propagation\n",
    "def backwardPropagation(X, Y, cache):\n",
    "\tm = X.shape[1]\n",
    "\t(Z1, A1, W1, b1, Z2, A2, W2, b2) = cache\n",
    "\t\n",
    "\tdZ2 = A2 - Y\n",
    "\tdW2 = np.dot(dZ2, A1.T) / m\n",
    "\tdb2 = np.sum(dZ2, axis = 1, keepdims = True)\n",
    "\t\n",
    "\tdA1 = np.dot(W2.T, dZ2)\n",
    "\tdZ1 = np.multiply(dA1, A1 * (1- A1))\n",
    "\tdW1 = np.dot(dZ1, X.T) / m\n",
    "\tdb1 = np.sum(dZ1, axis = 1, keepdims = True) / m\n",
    "\t\n",
    "\tgradients = {\"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "\t\t\t\t\"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "\treturn gradients\n",
    "\n",
    "# Updating the weights based on the negative gradients\n",
    "def updateParameters(parameters, gradients, learningRate):\n",
    "\tparameters[\"W1\"] = parameters[\"W1\"] - learningRate * gradients[\"dW1\"]\n",
    "\tparameters[\"W2\"] = parameters[\"W2\"] - learningRate * gradients[\"dW2\"]\n",
    "\tparameters[\"b1\"] = parameters[\"b1\"] - learningRate * gradients[\"db1\"]\n",
    "\tparameters[\"b2\"] = parameters[\"b2\"] - learningRate * gradients[\"db2\"]\n",
    "\treturn parameters\n",
    "\n",
    "# Model to learn the XOR truth table\n",
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]]) # XOR input\n",
    "Y = np.array([[0, 1, 1, 0]]) # XOR output\n",
    "\n",
    "# Define model parameters\n",
    "neuronsInHiddenLayers = 2 # number of hidden layer neurons (2)\n",
    "inputFeatures = X.shape[0] # number of input features (2)\n",
    "outputFeatures = Y.shape[0] # number of output features (1)\n",
    "parameters = initializeParameters(inputFeatures, neuronsInHiddenLayers, outputFeatures)\n",
    "epoch = 100000\n",
    "learningRate = 0.01\n",
    "losses = np.zeros((epoch, 1))\n",
    "\n",
    "for i in range(epoch):\n",
    "\tlosses[i, 0], cache, A2 = forwardPropagation(X, Y, parameters)\n",
    "\tgradients = backwardPropagation(X, Y, cache)\n",
    "\tparameters = updateParameters(parameters, gradients, learningRate)\n",
    "\n",
    "# Evaluating the performance\n",
    "# plt.figure()\n",
    "# plt.plot(losses)\n",
    "# plt.xlabel(\"EPOCHS\")\n",
    "# plt.ylabel(\"Loss value\")\n",
    "# plt.show()\n",
    "\n",
    "# Testing\n",
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]]) # XOR input\n",
    "cost, _, A2 = forwardPropagation(X, Y, parameters)\n",
    "prediction = (A2 > 0.5) * 1.0\n",
    "# print(A2)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdh-M7CTa8Nq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMDgtUEvJILGqChz0yIaVAz",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
